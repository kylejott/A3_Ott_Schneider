---
title: '**The Finnish Top 0.4 Percent**: An Exploration of Top Tax Shares in Finland from 2009 to 2013'
author: "Kyle Ott and Cornelius Schneider"
date: "14 November 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
bibliography:
- main1.bib
- packages1.bib
---
\pagebreak
```{r, include=FALSE}
pkgs <- c('httr', 'dplyr', 'XML', 'ggplot2', 'stringr', 'car', 'devtools', 'rsdmx', 'stargazer', 'knitr', 'CausalImpact' )
repmis::LoadandCite(pkgs, file = 'packages1.bib')



##################################
# Assignment 3: Data Science Course
# Kyle Ott & Cornelius Schneider
# 14 November 2014
##################################

# Load packages
library(httr)
library(dplyr)
library(XML)
library(ggplot2)
library(stringr)
library(car)
library(devtools)
library(rsdmx)
library(stargazer)
library(knitr)
library(CausalImpact)

#################
# Our Unique, Tidy, Open, Reproducible Data 
#################

## this part is scraping from the newspaper website for the years 2009 to 2013
```

```{r, include=FALSE, cache=TRUE}
# 2013 data
tables2013 = data.frame()

for (i in 1:30){

# URL with the ta table
URL_temp2013 <- paste0('http://www.taloussanomat.fi/verotiedot/2013/suurituloisimmat/?n=', i)
if (i==1) { tables2013 <- URL_temp2013 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
            tables2013 <- tables2013[[1]] }
else if (i!=1){

  # Gather content and parse all tables #
table_temp2013 <- URL_temp2013 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()

# Identify correct table
# names(table) # the table does not have an ID

# select first table with taxData
tables_df_temp2013 <- table_temp2013[[1]]

tables2013 <- rbind(tables2013, tables_df_temp2013)

}
#end loop
}
tables2013$year <- 2013

# 2012 data
tables2012 = data.frame()

for (i in 1:30){
  
  # URL with the medals table
  URL_temp2012 <- paste0('http://www.taloussanomat.fi/verotiedot/2012/suurituloisimmat/?n=', i)
  if (i==1) { tables2012 <- URL_temp2012 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
              tables2012 <- tables2012[[1]] }
  else if (i!=1){
    #### Gather content and parse all tables ####
    table_temp2012 <- URL_temp2012 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
    
    # Identify correct table
    # names(table) # the table does not have an ID
    
    # select first table with taxData
    tables_df_temp2012 <- table_temp2012[[1]]
    
    tables2012 <- rbind(tables2012, tables_df_temp2012)
  }
  ##end loop
}

tables2012$year <- 2012


## 2011 data
tables2011 = data.frame()

#note that for some years they don't have complete obs (15,000), which is why the loop is only up to 28 for the following year
for (i in 1:28){
  
  # URL with the medals table
  URL_temp2011 <- paste0('http://www.taloussanomat.fi/verotiedot/2011/suurituloisimmat/?n=', i)
  if (i==1) { tables2011 <- URL_temp2011 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
              tables2011 <- tables2011[[1]] }
  else if (i!=1){
    #### Gather content and parse all tables ####
    table_temp2011 <- URL_temp2011 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
    
    # Identify correct table
    # names(table) # the table does not have an ID
    
    # select first table with taxData
    tables_df_temp2011 <- table_temp2011[[1]]
    
    tables2011 <- rbind(tables2011, tables_df_temp2011)
  }
  #end loop
}

tables2011$year <- 2011

# 2010 data
tables2010 = data.frame()

for (i in 1:29){
  
  # URL with the medals table
  URL_temp2010 <- paste0('http://www.taloussanomat.fi/verotiedot/2010/suurituloisimmat/?n=', i)
  if (i==1) { tables2010 <- URL_temp2010 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
              tables2010 <- tables2010[[1]] }
  else if (i!=1){
    #### Gather content and parse all tables ####
    table_temp2010 <- URL_temp2010 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
        
    # select first table with taxData
    tables_df_temp2010 <- table_temp2010[[1]]
    
    tables2010 <- rbind(tables2010, tables_df_temp2010)
  }
  ##end loop
}

tables2010$year <- 2010

## 2009 data
tables2009 = data.frame()

for (i in 1:25){
  
  # URL with the medals table
  URL_temp2009 <- paste0('http://www.taloussanomat.fi/verotiedot/2009/suurituloisimmat/?n=', i)
  if (i==1) { tables2009 <- URL_temp2009 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
              tables2009 <- tables2009[[1]] }
  else if (i!=1){
    #### Gather content and parse all tables ####
    table_temp2009 <- URL_temp2009 %>% GET() %>% content(as = 'parsed') %>% readHTMLTable()
    
    # select first table with taxData
    tables_df_temp2009 <- table_temp2009[[1]]
    
    tables2009 <- rbind(tables2009, tables_df_temp2009)
  }
  ##end loop
}

tables2009$year <- 2009

#appending scraped tables
all <- rbind(tables2009, tables2010, tables2011, tables2012, tables2013)

save(all, file = "/Users/Kyle/Dropbox/!Fall_2014/Collab_Data/A3_Ott_Schneider/all.RData")


```

```{r, include=FALSE}
load("/Users/Kyle/Dropbox/!Fall_2014/Collab_Data/A3_Ott_Schneider/all.RData")

class(all$year)

#changing the titles to english
all <- plyr::rename(x = all,
                           replace = c("Nimi" = "name",
                                       "Tulot yht" = "total_inc",
                                       "Verot" = "taxes_paid",
                                       "Suhde" = "ratio"
                                       ))
str(all)

#cleaning ratio
all$ratio2 <- str_sub(all$ratio, 1, 2)
summary(all$ratio2)
all$ratio3 <- as.numeric(all$ratio2, length=2)
summary(all$ratio3)

#cleaning taxes_paid
sub(' â¬ $', '',all$taxes_paid)
all$taxes_paid2 <- sub(' â¬$', '',all$taxes_paid)
all$taxes_paid3 <- str_trim(all$taxes_paid2)
all$taxes_paid4 <-sub(' ', '',all$taxes_paid3)
all$taxes_paid5 <-sub(' ', '',all$taxes_paid4)
all$taxes_paid6 <- as.numeric(all$taxes_paid5, length=9)
summary(all$taxes_paid6)

#cleaning total_inc
all$total_inc2 <- sub(' â¬$', '',all$total_inc)
all$total_inc3 <- str_trim(all$total_inc2)
all$total_inc4 <-sub(' ', '',all$total_inc3)
all$total_inc5 <-sub(' ', '',all$total_inc4)
all$total_inc6 <- as.numeric(all$total_inc5, length=13)
summary(all$total_inc6)

#dropping name and keeping rank in given year
all$name2 <- str_sub(all$name, 1, 5)
all$name3 <- sub('\\. ..$', '',all$name2)
all$name4 <- sub('\\. .$', '',all$name3)
all$name5 <- sub('\\. $', '',all$name4)
all$name6 <- sub('\\.$', '',all$name5)
all$name7 <- as.numeric(all$name6, length=5)
summary(all$name7)

clean <- all[, (colnames(all) %in% c("name7", "total_inc6", "taxes_paid6", "ratio3", "year"))]
str(clean)
clean <- plyr::rename(x = clean,
                             replace = c("total_inc6" = "total_inc",
                                         "taxes_paid6" = "taxes_paid",
                                         "name7" = "rank",
                                         "ratio3" = "ratio"
                             ))

## now we have a clean and tidy dataset!


# add 1 to taxes paid if it is zero
clean$taxes_paid <- replace(clean$taxes_paid,clean$taxes_paid<=1, 1)

# log transforming the income variables
clean$log_taxes_paid <-log(clean$taxes_paid)
clean$log_total_inc <- log(clean$total_inc)

######## Scraping Macro Data

# OECD: Tax Revenues 2009 - 2012
# URL
URL <- 'http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/REV/NES.1000.TAXNAT.FIN?startTime=2009&endTime=2012'
sdmx <- readSDMX('http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/REV/NES.1000.TAXNAT.FIN?startTime=2009&endTime=2012')

# Data Frame
Tax09.12 <- as.data.frame(sdmx)

# Making It Tidy
drops <- c("GOV","TAX","TIME_FORMAT","Unit","PowerCode","VAR","COU")
Tax09.12 <- Tax09.12[,!(names(Tax09.12) %in% drops)]

as.numeric('obsTime', 'obsValue' )

names(Tax09.12)[1] <- "year"
names(Tax09.12)[2] <- "Total_Tax_Revenue"
Tax09.12$year <- as.numeric(Tax09.12$year)
Tax09.12$Total_Tax_Revenue <- as.numeric(Tax09.12$Total_Tax_Revenue)


# Tax Revenues 2013
Tax09.12$Total_Tax_Revenue <- Tax09.12$Total_Tax_Revenue*1000000000
Tax13 <- data.frame(year=2013, Total_Tax_Revenue =30780000000)
Tax09.13 <- rbind( Tax09.12, Tax13 )


# GDP in constant prices, national base year
Tax09.13$Total_GDP <- c(181664000000, 187100000000, 191910000000,189111000000,186831000000)

# Tax Rates & Delta Tax Rates & GDP
Tax09.13$Tax_Rate <- c(30.50, 30.00, 30.00, 29.75,31.75)

Tax09.13$DELTA_Tax_Rate <- c(NA, 0.5, 0,-0.25,1.0)


# Merge Our Scraped, Cleaned Data Sets
FINAL <- merge(clean, Tax09.13,
               by = c('year'))

#### Gathering more data

# OECD: Population Data 2009 - 2013
# URL
URL <- 'http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/POP_FIVE_HIST/FIN.YP99TLL1_ST.TT.A?startTime=2009&endTime=2013'
sdmx <- readSDMX('http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/POP_FIVE_HIST/FIN.YP99TLL1_ST.TT.A?startTime=2009&endTime=2013')

# Data Frame
Pop09.13 <- as.data.frame(sdmx)

# Making It Tidy
drops <- c("LOCATION","SUBJECT","SEX","FREQUENCY","TIME_FORMAT","Unit","OBS_STATUS")
Pop09.13 <- Pop09.13[,!(names(Pop09.13) %in% drops)]

as.numeric('obsTime', 'obsValue' )

names(Pop09.13)[1] <- "year"
names(Pop09.13)[2] <- "Population"
Pop09.13$year <- as.numeric(Pop09.13$year)
Pop09.13$Population <- as.numeric(Pop09.13$Population)

PercWorkPop <- c(0.6645439, 0.661943, 0.6570156, 0.6510898  , 0.6449715)
year <- c(2009, 2010, 2011, 2012, 2013)
WorkPop <- data.frame(year, PercWorkPop)

Pop09.13 <- merge(WorkPop, Pop09.13,
               by = c('year'))

Pop09.13$WorkPop <- Pop09.13$PercWorkPop*Pop09.13$Population 

drops <- c("PercWorkPop","Population")
Pop09.13 <- Pop09.13[,!(names(Pop09.13) %in% drops)]


# Merge Data Sets
FINAL <- merge(FINAL, Pop09.13,
               by = c('year'))

# Create Year Dummies
FINAL <- within(FINAL, yr2009<-ifelse(year==2009, 1, 0))
FINAL <- within(FINAL, yr2010<-ifelse(year==2010, 1, 0))
FINAL <- within(FINAL, yr2011<-ifelse(year==2011, 1, 0))
FINAL <- within(FINAL, yr2012<-ifelse(year==2012, 1, 0))
FINAL <- within(FINAL, yr2013<-ifelse(year==2013, 1, 0))

# Create Dep Var
#FINAL$total2009 <- with(FINAL, sum(FINAL[yr2009==1, "taxes_paid"]))
#FINAL$total2010 <- with(FINAL, sum(FINAL[yr2010==1, "taxes_paid"])) 
#FINAL$total2011 <- with(FINAL, sum(FINAL[yr2011==1, "taxes_paid"]))  
#FINAL$total2012 <- with(FINAL, sum(FINAL[yr2012==1, "taxes_paid"])) 
#FINAL$total2013 <- with(FINAL, sum(FINAL[yr2013==1, "taxes_paid"])) 

#FINAL$share2009 <- FINAL$total2009/FINAL$Total_Tax_Revenue
#FINAL$share2010 <- FINAL$total2010/FINAL$Total_Tax_Revenue 
#FINAL$share2011 <- FINAL$total2011/FINAL$Total_Tax_Revenue 
#FINAL$share2012 <- FINAL$total2012/FINAL$Total_Tax_Revenue 
#FINAL$share2013 <- FINAL$total2013/FINAL$Total_Tax_Revenue 

share <- c(0.04898281, 0.06022747, 0.06752648, 0.06347982, 0.0770184)
year <- c(2009, 2010, 2011, 2012, 2013)
shares <- data.frame(year, share)


FINAL <- merge(FINAL, shares,
               by = c('year'))


################
#Descriptive Statistics
################

# createing subsets just for further analysis and ease of calculations
FINAL2009 <- subset(FINAL, year == 2009)
FINAL2010 <- subset(FINAL, year == 2010)
FINAL2011 <- subset(FINAL, year == 2011)
FINAL2012 <- subset(FINAL, year == 2012)
FINAL2013 <- subset(FINAL, year == 2013)

# number of observations by year
obs <- tally(group_by(FINAL, year))

# creating first summary statistics table
sum2_table <- merge(obs, Pop09.13,
                             by = c('year'))
percent <- (obs$n / Pop09.13$WorkPop)*100
percent_of_working <- data.frame(year, percent)

sum2_table <- merge(sum2_table, percent_of_working,
                    by = c('year'))
sum2_table <- merge(sum2_table, Tax09.13,
                    by = c('year'))
```

#Introduction

The question about an “optimal” income taxation was always discussed against the background of classic economic theories: Income taxation should maximize a given social welfare function, that depicts a societies preference for equality. Furthermore, sacrifice theory of income taxation illustrates that redistribution should take place up to the point where marginal utilities  are equalized. 

However, these theories completely neglect behavioral responses to taxation. According to the Laffer-Curve at one point a further increase of the tax rate would result in a loss in tax revenues due to negative labor supply responses.
In a very recent work, [@piketty2014inequality] identifies additional reasons, why trends in top-income shares are correlated with the tax rates: Labor responses, evasion/avoidance responses and bargaining responses. 

Whereas poverty is studied extensively in Economics through surveys and welfare programs, the current debate still lacks information about the top of the income distribution. The aim of this project is to inspect exactly this upper end of the income distribution. For this, we’ll analyze micro-level data from finish taxpayers of the years 2009 to 2013. 

In Finland the tax on earned income is levied according to a progressive tax scale: Each tax payer has to pay a basic amount dependent on his earned income plus the tax rate within the respective tax bracket. The concrete tax scheme is decided annually by the Parliament. The relevant tax rates in the top tax-bracket were levied as follows: 

* 2009: 30.5%
* 2010: 30.0%
* 2011: 30.0%
* 2012: 29.75%
* 2013: 31.75%

Against this background, our paper’s purpose is to identify anomalies in the tax patterns of ultra-wealthy finish people. 
In a first step, we will visualize how the share of the top 0.4% (wealthiest 15.000 individuals) in total income-tax revenue changes between 2009 and 2013. Following up, with inferential statistics we try to dig deeper into income-tax payer’s behavior: What is the predicted probability to pay a certain average tax rate given your income? What was the potential impact of changes in top tax-rates on ultra-wealthy finish people?
Conditional on the first results, more precise models, as well as a suitable strategy to tackle the raised questions will be assessed. 

# Our Unique, Tidy, Open, Reproducible Dataset
The Nordic countries Finland, Sweden and Norway have a tradition of publishing everyone’s income and tax details every year. Whereas in Sweden and Norway this data is only accessible being a citizen and after pulling an official request, in Finland top income tax earners are public figures as a result of heightened media scrutiny on top income tax earners. Finland’s largest business online daily newspaper “Taloussanomat“ published the figures in a suitable format for our purpose: The top 15.000 income earners are displayed on yearly basis from 2009 to 2013, including their total income, taxed paid and average tax rate. 

In a first step we scraped the data from their homepage. Given that each year’s data was partitioned into 30 tables of 500 observations each, we wrote a loop in order to gather the date in one data frame. We ran that loop for each year and appended them into the same data frame. Moreover, we converted the strings with weird characters and various lengths into numeric class. Since there are some observations missing in the first three years (2009 to 2011), the final data set contains 70.402 observations.

Afterwards, we merged that data frame with two tables from the OECD: One indicating the total Finnish income-tax revenues from 2009 to 2013 and one indicating the number of working population. In addition to that, we manually amended the total GDP in constant prices (national base year) as well as the respective top-tax rates for each year. 

Based on this data, we created a couple of new variables that are helpful for our analysis: The rank based on the income of each individual in every year; the change in the tax rate compared to the previous year and the share of total income-revenues that the top 15.000 tax payers pay.

Before we continue to the descriptive analysis, just one note on the origins of our various data. First, as already mentioned, all of the micro-level income data comes from taloussanomat, the income tax revenue for 2013 comes from @incometaxrev2013, the income tax revenue for 2009-2012 comes from @oecdstatsinctax0912, the population data comes from @oecdstatspopulation, the income tax scheme for 2013 comes from @nordisketax, the income tax scheme for 2012 comes from @kpmg, the income tax scheme for 2011 comes from @wikipediaTax, our income tax scheme for 2010 comes from @crowe2010worldwide, and the income tax scheme for 2009 comes from @tax2009finland.

# Descriptive Statistics of Our Data
In the table below, you will see the breakdown of the data by: year, number of observation ("n"), total working population in Finland in given year, share in working population presented by top 15,000 income earners, total tax revenues collected (in Euros), total GDP (in constant prices national base year), top income tax rate, and the change of this tax rate by year.


```{r, results='asis', echo=FALSE}
# This is our first table
knitr::kable(sum2_table, align ='c', digits = c(4,5,7,2,12,13,4,3))
```

The table below shows a quick summary statistics about the distribution of our data and three key variables of interest over all five years: total annual income, total paid taxes, and the average tax rate. 

```{r, include=FALSE}
# Creating our Second Summary Table
obs_all <- tally(FINAL)
mean_inc <- mean(FINAL$total_inc)
mean_tax <- mean(FINAL$taxes_paid)
mean_ratio <- mean(FINAL$ratio)
med_inc <- median(FINAL$total_inc)
med_tax <- median(FINAL$taxes_paid)
med_ratio <- median(FINAL$ratio)
sd_inc <- sd(FINAL$total_inc)
sd_tax <- sd(FINAL$taxes_paid)
sd_ratio <- sd(FINAL$ratio)

tot_inc <- c(med_inc, mean_inc, sd_inc)
tot_paid_taxes <- c(med_tax, mean_tax, sd_tax)
ave_tax_rate <- c(med_ratio, mean_ratio, sd_ratio)

# creating table labels
table3 <- c('Median', 'Mean', 'SD')
sum3_table <- data.frame(table3, tot_inc, tot_paid_taxes, ave_tax_rate)
```


```{r, results='asis', echo=FALSE}
knitr::kable(sum3_table, align ='c', digits = c(0,4,4,1))
```

The figure below shows the range of average tax rates paid each year across all observations. It was included to visually display the range as it slightly changes across time.



```{r, results='asis', echo=FALSE}
# shows the average tax rate paid over the years, see that the range doesn't change all too much
qplot(year, ratio, data=FINAL, main='Average Tax Rate Paid Across Years', ylab ='Average Tax Rate')
```

The next plot was included to show the total income outliers that are very clear in 2013 (top right corner). In future analysis, we will consider dropping these extreme outliers (recent techonology millionaires).

```{r, results='asis' , echo=FALSE}
# shows the income outliers in 2013; Q: should we drop them?
qplot(year, total_inc, data=FINAL, main="Total Income by Year Highlighting 2013 Outliers", ylab = 'Total Annual Income')
```

The next plot shows the density plot of the average tax rates for all years. It is interesting to note that there is a bimodal distribution. We cannot make any inferences about why this may be the case, because we are not able to observe actual behavoir responses due to data limitations. Moreover, almost all of the observations fall between roughly 25 and 55 percent.

```{r, results='asis', echo=FALSE}
# plotting the density of the average tax rate
d <- density(FINAL$ratio) # returns the density data 
plot(d, main = 'Density Plot of Average Tax Rates') # plots the results
```

The plot below shows the histogram of the total annual income after it has been logged. Before it was lot it was extremely far from normal, now it is a bit closer but still not ideal. However, we are aware that our data suffers from skew because we are focusing only on a specific subsample, namely the ultra wealthy.

```{r, message=FALSE, results='asis', echo=FALSE}
# hist(FINAL$total_inc, main = '')
# income looks slightly better after logging it
hist(FINAL$log_total_inc, main = 'Histogram of Logged Total Income', xlab = 'Log Total Annual Income')
```

The next figure plots the log total annual income against the average tax rate. Here, in our visual outlook of our data, we are interested in observing patterns between these two key variables. There are clearly multiple instances where higher income individuals have a small average tax rate, which is likely due to potential tax avoidance like for example, shifting income to capital gains. This figure is one in which we would like to develop further for our final project.

```{r, results='asis', echo=FALSE}
# plotting income against tax rate
qplot(ratio, log_total_inc, data=FINAL, ylab  = 'Log Total Annual Income', xlab ='Average Tax Rate', main = "Plot of Average Tax Rate Against Annual Total Income")
```

The following plot is a zoomed in version of the previous plot. It shows the frequency of tax payers who are named above and likely engaging in tax avoidance schemes and resulting with low average tax rates.


```{r, results='asis', message=FALSE, echo=FALSE}
qplot(ratio, log_total_inc, data=FINAL, xlim=c(0,31), ylab  = 'Log Total Annual Income', xlab ='Average Tax Rate', main = "Plot of Average Tax Rate < 31% Against Log Annual Total Incomes")
```

The line graph depicts the development of the share of the total income tax revenue paid by the top 0.4 percent, which is our sample. Over the course of five years, the share increase from 5 percent to almost 8 percent. An in each year, we can nicely observe the changes in the top income tax rate changes which likely resulted in the reflected changed in tax revenue collected by the top 0.4 percent.

```{r, results='asis', echo=FALSE}
sharefigure <- qplot(shares$year, shares$share, caption='Top 0.4% Share of Total Paid Finnish Taxes', ylim=c(0.04, 0.08), geom='line', ylab='Total Finnish Taxes Share Paid by Top 0.4%', xlab='Year')
sharefigure + theme_bw(base_size = 13)
```

# Inferential Statistics: First Model Attempts
```{r, include=FALSE}
FINAL <- within(FINAL, less30<-ifelse(ratio<30, 1, 0))
logit1 <- glm(less30 ~ log_total_inc, data = FINAL, family = 'binomial')
summary(logit1)
confint(logit1)
fitted <- with(FINAL,data.frame(log_total_inc))
fitted$predicted <- predict(logit1, newdata = fitted, type = 'response')
```

Our first attempt at running a model is extremely simple. Essentially, we want to see and develop in further analysis a relationship between a given income and the predicted probabilty of paying an average tax rate below certain thresholds. In our first attempt, we ran a logit model with a binary dependent variable (with 1 being observations with a average tax rate below 30). The results are best illustrated by the following figure, a predicted probability plot. The curve of the probabilities is intriguing. The maximum income has the highest probability of paying a tax rate below 30 percent. This is a strong result. Moreover, the curve suggests that lower incomes (within our sample) have a lower probability of paying an adventageous average tax rate. This finding can be seen as motivation for further inspection and analysis.


```{r, results='asis', echo=FALSE}
qplot(fitted$predicted, log_total_inc, data = FINAL, xlab = 'Predicted Probability', ylab = 'Log Total Income', main='Predicted Probability of Having an Average Tax Rate Below 30' )
```

The (simple) "dynamic" regression output below is our failed attempt to run a legitimate and appropriate OLS model including our macro data. Going forward, we hope to find a way to run an model that is able to overcome our initial difficulties. 

```{r, inlcude=FALSE, echo=FALSE}
M1 <- lm(share ~ Tax_Rate+ratio+DELTA_Tax_Rate+log(Total_GDP),data = FINAL)
# summary(M1)

# Create cleaner covariate labels
labels <- c('(Intercept)', 'Top tax rate', 'Change in top tax rate',
            'Log total GDP')
```

```{r, message=FALSE, results='asis', echo=FALSE}
stargazer::stargazer(M1, covariate.labels = labels,
                     title = 'Inappropriate Model, First Try',
                     type = 'latex', header = FALSE)
```


# Conclusion: Further Analysis Needed
Our attempt to analyze the top income-tax share dependent on the tax rate by running an ordinary least squares model has failed so far. Obviously controlling for GDP and considering year dummies didn’t lead to the expected outcome: We’re still considering to incorporate other data to build a worthy model. Finally, perhaps our focus should shift to expanding on our simple logit model and incorporating other variables and also tweaking our depending variable looking at different thresholds and also different years.


***

This project used @CiteRStudio to create this assignment.

\pagebreak

# References